{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Run the capstoneETL.py as show below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created session\n",
      "Created Client: s3\n",
      "Created Resource: s3\n",
      "Creating new S3 Bucket\n",
      "Uploading source files\n",
      "Uploading source files\n",
      "Uploading source files\n",
      "Start process_state_ref\n",
      "State data has been read into a Pyspark df\n",
      "Start process_babyName_fact\n",
      "Baby name data has been read into a Pyspark df\n",
      "Start process_stormsByName_dim\n",
      "State data has been read into a Pyspark df\n",
      "Start process_stormsLocation_dim\n",
      "+----------+\n",
      "|storm_date|\n",
      "+----------+\n",
      "|1950-08-12|\n",
      "|1950-08-12|\n",
      "|1950-08-12|\n",
      "|1950-08-12|\n",
      "|1950-08-13|\n",
      "|1950-08-13|\n",
      "|1950-08-13|\n",
      "|1950-08-13|\n",
      "|1950-08-14|\n",
      "|1950-08-14|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Storm data has been read into a Pyspark df\n",
      "Start process_stormsSeverity_dim\n",
      "+----------+\n",
      "|storm_date|\n",
      "+----------+\n",
      "|1950-08-12|\n",
      "|1950-08-12|\n",
      "|1950-08-12|\n",
      "|1950-08-12|\n",
      "|1950-08-13|\n",
      "|1950-08-13|\n",
      "|1950-08-13|\n",
      "|1950-08-13|\n",
      "|1950-08-14|\n",
      "|1950-08-14|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Storm data has been read into a Pyspark df\n",
      "Start process_stormsMetadata_fact\n",
      "+----------+\n",
      "|storm_date|\n",
      "+----------+\n",
      "|1950-08-12|\n",
      "|1950-08-12|\n",
      "|1950-08-12|\n",
      "|1950-08-12|\n",
      "|1950-08-13|\n",
      "|1950-08-13|\n",
      "|1950-08-13|\n",
      "|1950-08-13|\n",
      "|1950-08-14|\n",
      "|1950-08-14|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Storm data has been read into a Pyspark df\n",
      "Start process_stormsBabyNames_fact\n",
      "+----------+\n",
      "|storm_date|\n",
      "+----------+\n",
      "|1950-08-12|\n",
      "|1950-08-12|\n",
      "|1950-08-12|\n",
      "|1950-08-12|\n",
      "|1950-08-13|\n",
      "|1950-08-13|\n",
      "|1950-08-13|\n",
      "|1950-08-13|\n",
      "|1950-08-14|\n",
      "|1950-08-14|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Storm data has been read into a Pyspark df\n",
      "['babyName_id', 'birth_year', 'popularity', 'birth_state_code', 'gender', 'birth_name']\n",
      "['storm_meta_id', 'babyName_id', 'storm_id', 'storm_name', 'birth_name', 'gender', 'birth_year', 'popularity', 'storm_year', 'category', 'storm_state_code', 'birth_state_code', 'storm_babyName_id']\n",
      "sparkify etl has completed successfully           duration:  458.418922662735\n"
     ]
    }
   ],
   "source": [
    "%run capstoneETL.py\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Alternate way to run, if the above does not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### copy the capstoneETL.py code expect the below code\n",
    "```\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Runs full ETL pipeline\n",
    "    \"\"\"\n",
    "    session = createBoto3Session()\n",
    "    s3_client = createSessionClients(session,'s3', REGION)\n",
    "    s3_resource = createSessionResources(session, 's3', REGION)\n",
    "    create_new_S3(s3_client, SOURCE_BUCKET, REGION)\n",
    "    \n",
    "    upload_multiple_files_toS3Folder(s3_client, 'source_namesbystate/', '*.txt', SOURCE_BUCKET, 'SOURCE/')\n",
    "    upload_multiple_files_toS3Folder(s3_client, 'source_weather/', '*.csv', SOURCE_BUCKET, 'SOURCE/')\n",
    "    upload_multiple_files_toS3Folder(s3_client, 'source_states/', '*.json', SOURCE_BUCKET, 'SOURCE/')\n",
    "    \n",
    "    spark = create_spark_session()\n",
    "    region = config.get(\"S3\", 'region')\n",
    "\n",
    "    starttime = time.time()\n",
    "    print(\"Start process_state_ref\")\n",
    "    process_state_ref(spark,SOURCE_BUCKET)\n",
    "    print(\"Start process_babyName_fact\")\n",
    "    babyNames_byState_fact = process_babyName_fact(spark,SOURCE_BUCKET)\n",
    "    print(\"Start process_stormsByName_dim\")\n",
    "    process_stormsByName_dim(spark,SOURCE_BUCKET)\n",
    "    print(\"Start process_stormsLocation_dim\")\n",
    "    process_stormsLocation_dim(spark,SOURCE_BUCKET)\n",
    "    print(\"Start process_stormsSeverity_dim\")\n",
    "    process_stormsSeverity_dim(spark,SOURCE_BUCKET)\n",
    "    print(\"Start process_stormsMetadata_fact\")\n",
    "    storms_metadata_fact = process_stormsMetadata_fact(spark,SOURCE_BUCKET)\n",
    "    print(\"Start process_stormsBabyNames_fact\")\n",
    "    process_stormsBabyNames_fact(spark,SOURCE_BUCKET,storms_metadata_fact,babyNames_byState_fact)\n",
    "    \n",
    "    print('sparkify etl has completed successfully \\\n",
    "          duration: ', time.time() - starttime)\n",
    "    \n",
    "    #delete_S3(s3_resource, SOURCE_BUCKET)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf, col, desc, asc\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import year, month, dayofmonth, \\\n",
    "    hour, weekofyear, date_format\n",
    "from pyspark.sql.functions import isnan, col, when, count\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType as R, \\\n",
    "            StructField as Fld, \\\n",
    "            DoubleType as Dbl,  \\\n",
    "            LongType as Long,   \\\n",
    "            StringType as Str,  \\\n",
    "            IntegerType as Int, \\\n",
    "            DecimalType as Dec, \\\n",
    "            DateType as Date,   \\\n",
    "            FloatType as Float, \\\n",
    "            TimestampType as Stamp\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "import configparser \n",
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import glob\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('capstone.cfg')\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = config[\"AWS\"][\"AWS_ACCESS_KEY_ID\"]\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config[\"AWS\"][\"AWS_SECRET_ACCESS_KEY\"]\n",
    "REGION = config[\"AWS\"][\"REGION\"]\n",
    "SOURCE_BUCKET = config[\"S3\"][\"SOURCE_BUCKET\"]\n",
    "\n",
    "\n",
    "\n",
    "def createBoto3Session(): \n",
    "    \"\"\"\n",
    "    create boto3 session\n",
    "    params: profile can be set \n",
    "    or defaults to 'default'\n",
    "    \"\"\"\n",
    "#     import boto3\n",
    "    session = boto3.session.Session()    \n",
    "    print('Created session')\n",
    "    return session\n",
    "\n",
    "\n",
    "def createSessionClients(session, awsService, REGION):  \n",
    "    \"\"\"\n",
    "    create boto3 client \n",
    "    from a session.\n",
    "    \n",
    "    params: aws service and region  \n",
    "    \"\"\"\n",
    "#     import boto3\n",
    "    client = session.client(awsService,\n",
    "                          region_name= REGION)\n",
    "    print(\"Created Client: \" + awsService)\n",
    "    return client\n",
    "\n",
    "\n",
    "def createSessionResources(session, awsService, REGION): \n",
    "    \"\"\"\n",
    "    creates boto3 resource\n",
    "    from a session\n",
    "    \n",
    "    params: aws service and region\n",
    "    \"\"\"\n",
    "#     import boto3\n",
    "    resource = session.resource(awsService, region_name= REGION)\n",
    "    print(\"Created Resource: \" + awsService)    \n",
    "    return resource\n",
    "\n",
    "\n",
    "def create_new_S3(s3_client, BUCKET, REGION):\n",
    "    \"\"\"\n",
    "    create a new S3 bucket\n",
    "    \n",
    "    params: bucket name and region\n",
    "    \"\"\"\n",
    "    print(\"Creating new S3 Bucket\")\n",
    "    try: \n",
    "        s3_new = s3_client.create_bucket(\n",
    "                    ACL= 'public-read-write',\n",
    "                    Bucket= BUCKET,\n",
    "                    CreateBucketConfiguration={\n",
    "                        'LocationConstraint': REGION})\n",
    "         \n",
    "        return s3_new\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "        \n",
    "def upload_multiple_files_toS3Folder(s3_client, PATH, KEY, BUCKET, folder):  \n",
    "    \"\"\"\n",
    "    using the boto3 s3 client\n",
    "    loops through a directory for file types\n",
    "    uploads to an S3 bucket by partition\n",
    "    \"\"\"\n",
    "#     import os\n",
    "#     import glob    \n",
    "    \n",
    "    print(\"Uploading source files\")    \n",
    "    files = glob.glob(PATH + KEY)  \n",
    "\n",
    "    try: \n",
    "        for f in files: \n",
    "            upload = s3_client.upload_file(\n",
    "                Filename = f,\n",
    "                Bucket = BUCKET,\n",
    "                Key = folder + f.split(\"/\")[-1])\n",
    "\n",
    "        return upload\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e) \n",
    "\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Create a spark session\n",
    "    \"\"\"\n",
    "    os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "    os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "    os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "    os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "    spark = SparkSession.builder.config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0,saurfang:spark-sas7bdat:2.0.0-s_2.11\").enableHiveSupport().getOrCreate()\n",
    "    return spark\n",
    "   \n",
    "      \n",
    "        \n",
    "def readStateData(SOURCE_BUCKET=''):\n",
    "    \"\"\"\n",
    "    A helper function created to read json state data from a \\\n",
    "    newly created s3 bucket apply data transformations and\\\n",
    "    output the data as a Pyspark dataframe.\n",
    "    \"\"\"\n",
    "    spark = create_spark_session()\n",
    "    spark.conf.set(\"spark.sql.legacy.json.allowEmptyString.enabled\", True)\n",
    "    \n",
    "    # read state data\n",
    "    states_data = spark.read \\\n",
    "    .option(\"multiline\",\"true\")\\\n",
    "    .json('s3a://'+SOURCE_BUCKET+'/SOURCE/states.json')\n",
    "\n",
    "    # drop unneeded column\n",
    "    states_data = states_data.drop(col('Abbrev'))\n",
    "    \n",
    "    # rename and re-case remaining headers\n",
    "    states_data = states_data \\\n",
    "    .withColumnRenamed(\"State\", \"state_name\") \\\n",
    "    .withColumnRenamed(\"Code\", \"state_code\") \n",
    "    \n",
    "    # create an 'index'\n",
    "    states_data = states_data \\\n",
    "        .withColumn(\"state_id\", \\\n",
    "                    F.monotonically_increasing_id())\n",
    "\n",
    "    win = W.orderBy(\"state_id\")\n",
    "\n",
    "    states_data = states_data \\\n",
    "        .withColumn(\"state_id\", \\\n",
    "                    F.row_number().over(win)-1)\n",
    "    \n",
    "    print('State data has been read into a Pyspark df')\n",
    "    return states_data\n",
    "\n",
    "\n",
    "def readBabyNameData(SOURCE_BUCKET=''):\n",
    "    \"\"\"\n",
    "    A helper function created to read baby name data in \\\n",
    "    50 separate text files from a newly created s3 \\\n",
    "    bucket apply data transformations and\n",
    "    output the data as a Pyspark dataframe.\n",
    "    \"\"\"\n",
    "    spark = create_spark_session()\n",
    "\n",
    "    # read baby name data\n",
    "    namesbystate = spark.read \\\n",
    "    .option(\"header\",False) \\\n",
    "    .text('s3a://'+SOURCE_BUCKET+'/SOURCE/*.txt')\n",
    "    \n",
    "    # use split to make headers from existing columns\n",
    "    split_col = \\\n",
    "        F.split(namesbystate['value'], ',')\n",
    "\n",
    "    namesbystate = namesbystate \\\n",
    "        .withColumn('birth_year', \\\n",
    "                    split_col.getItem(2) \\\n",
    "                   )\n",
    "\n",
    "    namesbystate = namesbystate \\\n",
    "        .withColumn('popularity', \\\n",
    "                    split_col.getItem(4) \\\n",
    "                   )\n",
    "\n",
    "    namesbystate = namesbystate \\\n",
    "     .withColumn('birth_state_code', \\\n",
    "                 split_col.getItem(0) \\\n",
    "                )\n",
    "\n",
    "    namesbystate = namesbystate \\\n",
    "        .withColumn('gender', \\\n",
    "                    split_col.getItem(1) \\\n",
    "                   )\n",
    "\n",
    "    namesbystate = namesbystate \\\n",
    "        .withColumn('birth_name', \\\n",
    "                    split_col.getItem(3) \\\n",
    "                   ) \\\n",
    "\n",
    "    #drop unneeded column\n",
    "    namesbystate = namesbystate.drop('value')\n",
    "\n",
    "     # trim leading and/or trailing spaces\n",
    "    namesbystate = namesbystate\\\n",
    "        .withColumn('birth_state_code', \\\n",
    "                    trim(namesbystate['birth_state_code']))\n",
    "\n",
    "    namesbystate = namesbystate\\\n",
    "        .withColumn('gender', \\\n",
    "                    trim(namesbystate['gender']))\n",
    "\n",
    "    namesbystate = namesbystate\\\n",
    "        .withColumn('birth_year', \\\n",
    "                    trim(namesbystate['birth_year']))\n",
    "\n",
    "    namesbystate = namesbystate\\\n",
    "     .withColumn('birth_name', \\\n",
    "                trim(namesbystate['birth_name'])) \\\n",
    "\n",
    "    namesbystate = namesbystate\\\n",
    "     .withColumn('popularity', \\\n",
    "                trim(namesbystate['popularity']))     \n",
    "    \n",
    "    # make birth_name lowercase\n",
    "    namesbystate = namesbystate\\\n",
    "     .withColumn('birth_name', \\\n",
    "                lower(col('birth_name')))\n",
    "    \n",
    "    # sort in ascending order\n",
    "    namesbystate = \\\n",
    "        namesbystate.sort( \\\n",
    "        namesbystate['birth_year'].asc(), \\\n",
    "        namesbystate['birth_name'].asc(),\n",
    "        namesbystate['popularity'].asc(), \\\n",
    "        namesbystate['birth_state_code'].asc())\n",
    "    \n",
    "    # create an 'index'\n",
    "    namesbystate = namesbystate \\\n",
    "        .withColumn(\"babyName_id\", \\\n",
    "                    F.monotonically_increasing_id())\n",
    "\n",
    "    win = W.orderBy(\"babyName_id\")\n",
    "\n",
    "    namesbystate = namesbystate \\\n",
    "        .withColumn(\"babyName_id\", \\\n",
    "                    F.row_number().over(win)-1)\n",
    "    \n",
    "    #reorder df\n",
    "    namesbystate = namesbystate\\\n",
    "        .select('babyName_id', \\\n",
    "                'birth_year', \\\n",
    "                'popularity', \\\n",
    "                'birth_state_code', \\\n",
    "                'gender', \\\n",
    "                'birth_name')\n",
    "\n",
    "\n",
    "    print('Baby name data has been read into a Pyspark df')\n",
    "    return namesbystate\n",
    "\n",
    "\n",
    "def readStormData(SOURCE_BUCKET=''):\n",
    "    \"\"\"\n",
    "    A helper function created to read baby name data in \\\n",
    "    50 separate text files from a newly created s3 \\\n",
    "    bucket apply data transformations and\n",
    "    output the data as a Pyspark dataframe.\n",
    "    \"\"\"\n",
    "    spark = create_spark_session()\n",
    "\n",
    "    # read storm data\n",
    "    fromStormsCSV = spark.read \\\n",
    "        .option(\"header\", True) \\\n",
    "        .option(\"ignoreTrailingWhiteSpace\",True) \\\n",
    "        .option(\"ignoreLeadingWhiteSpace\", True) \\\n",
    "        .csv('s3a://'+SOURCE_BUCKET+'/SOURCE/storms_data.csv')\n",
    "    \n",
    "    # trim leading and/or trailing spaces\n",
    "    fromStormsCSV = fromStormsCSV\\\n",
    "        .withColumn('storm_id', \\\n",
    "                    trim(col('storm_id'))) \\\n",
    "        .withColumn('storm_name', \\\n",
    "                    trim(col('storm_name')), \\\n",
    "                   ) \\\n",
    "        .withColumn('associated_records', \\\n",
    "                    trim(col('associated_records')) \\\n",
    "                   ) \\\n",
    "        .withColumn('storm_time', \\\n",
    "                    trim(col('storm_time'))) \\\n",
    "        .withColumn('rec_identifier', \\\n",
    "                    trim(col('rec_identifier'))) \\\n",
    "        .withColumn('storm_type', \\\n",
    "                    trim(col('storm_type'))) \\\n",
    "        .withColumn('latitude', \\\n",
    "                    trim(col('latitude'))) \\\n",
    "        .withColumn('longitude', \\\n",
    "                    trim(col('longitude'))) \\\n",
    "        .withColumn('max_sustained_wind(kt)', \\\n",
    "                    trim(col('max_sustained_wind(kt)')) \\\n",
    "                   ) \\\n",
    "        .withColumn('minimum_pressure(mbar)', \\\n",
    "                    trim(col('minimum_pressure(mbar)'))) \n",
    "    \n",
    "    # change to lowercase\n",
    "    fromStormsCSV = fromStormsCSV\\\n",
    "        .withColumn('storm_name', \\\n",
    "            lower(col('storm_name')))\n",
    "    \n",
    "    # recast to str fields to int\n",
    "    fromStormsCSV = fromStormsCSV \\\n",
    "        .withColumn(\"associated_records\", \\\n",
    "            fromStormsCSV[\"associated_records\"] \\\n",
    "        .cast(IntegerType()))\n",
    "\n",
    "    fromStormsCSV = fromStormsCSV\\\n",
    "        .withColumn(\"max_sustained_wind(kt)\",\\\n",
    "            fromStormsCSV[\"max_sustained_wind(kt)\"]\\\n",
    "        .cast(IntegerType()))\n",
    "    \n",
    "    # split storm_date into new column for storm_year\n",
    "    fromStormsCSV.select('storm_date').show(10,False)\n",
    "    split_col = F.split(fromStormsCSV['storm_date'], '-') \n",
    "\n",
    "    fromStormsCSV = fromStormsCSV \\\n",
    "        .withColumn('storm_year', \\\n",
    "                    trim(split_col.getItem(0)))\n",
    "    \n",
    "    # create basin and ATCF_cyclone_num_forYear from \\\n",
    "    # the storm_id using substring\n",
    "    fromStormsCSV = fromStormsCSV\\\n",
    "        .withColumn('ATCF_cyclone_num_forYear', \\\n",
    "                    col('storm_id').substr(3, 2))\\\n",
    "        .withColumn('basin',col('storm_id').substr(1, 2))\n",
    "    \n",
    "    # filter for named storms\n",
    "    namedStorms = fromStormsCSV \\\n",
    "            .filter(col('storm_name')!= 'unnamed')\n",
    "    \n",
    "    \n",
    "    # change data type for max_sustained_wind(kt) to float\n",
    "    namedStorms = namedStorms \\\n",
    "        .withColumn('max_sustained_wind(kt)' \\\n",
    "            ,(namedStorms['max_sustained_wind(kt)']).cast(Float()))\n",
    "    \n",
    "    print('Storm data has been read into a Pyspark df')\n",
    "    return namedStorms\n",
    "         \n",
    "    \n",
    "def process_saffir_simpson_hurricane_wind_scale_ref(spark,SOURCE_BUCKET=''):\n",
    "    \"\"\"\n",
    "    NURDAT2 data contains no reference to\n",
    "    category. In order to identify category, \n",
    "    the Saffir_Simpson Wind Scale must be \n",
    "    referenced. The max_sustained_wind(kt)\n",
    "    can be measured against the min-max of \n",
    "    each category's range to set the \n",
    "    category, where the storm_type is \n",
    "    hurricane\n",
    "\n",
    "    args:\n",
    "    spark = spark session parameters\n",
    "    \"\"\"\n",
    "\n",
    "    spark = create_spark_session()\n",
    "    \n",
    "    data = [{'category': 1, \n",
    "             'sustained_wind(kt)': '64-82', \n",
    "             'max_sustained_wind(kt)': 82, \n",
    "             'min_sustained_wind(kt)': 64,\n",
    "             'sustained_wind(mph)': '74-95', \n",
    "             'brief_damage_description': \\\n",
    "             'Power outages that could last a few to several days.'},\n",
    "\n",
    "           {'category': 2, \n",
    "            'sustained_wind(kt)': '83-95', \n",
    "            'max_sustained_wind(kt)': 95, \n",
    "            'min_sustained_wind(kt)': 83,\n",
    "            'sustained_wind(mph)': '96-110', \n",
    "            'brief_damage_description': \\\n",
    "            'Near-total power loss is expected \\\n",
    "            with outages that could last from several days to weeks.'},\n",
    "\n",
    "           {'category': 3, \n",
    "            'sustained_wind(kt)': '96-112', \n",
    "            'max_sustained_wind(kt)': 112, \n",
    "            'min_sustained_wind(kt)': 96,\n",
    "            'sustained_wind(mph)': '111-129', \n",
    "            'brief_damage_description': \\\n",
    "            'Electricity and water will be \\\n",
    "            unavailable for several days to weeks after the storm passes.'},\n",
    "\n",
    "           {'category': 4,\n",
    "            'sustained_wind(kt)': '113-136', \n",
    "            'max_sustained_wind(kt)': 136, \n",
    "            'min_sustained_wind(kt)': 113,\n",
    "            'sustained_wind(mph)': '130-156', \n",
    "            'brief_damage_description': \\\n",
    "            'Catastrophic damage will occur; most of \\\n",
    "            the area will be uninhabitable for weeks or months.'},\n",
    "\n",
    "           {'category': 5,\n",
    "            'sustained_wind(kt)': '137+', \n",
    "            'min_sustained_wind(kt)': 137, \n",
    "            'sustained_wind(mph)': '157+',\n",
    "            'brief_damage_description': \\\n",
    "            'Catastrophic damage will occur; most of the \\\n",
    "            area will be uninhabitable for weeks or months.'}]\n",
    "\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField('category', Int()),\n",
    "        StructField('min_sustained_wind(kt)', Int()),\n",
    "        StructField('max_sustained_wind(kt)', Int()),\n",
    "        StructField('sustained_wind(kt)', Str()),\n",
    "        StructField('brief_damage_description', Str())\n",
    "    ])\n",
    "\n",
    "    # create data frame\n",
    "    saffir_simpson_scale = spark.createDataFrame(data, schema)\n",
    "#     print(saffir_simpson_scale.printSchema)\n",
    "    saffir_simpson_scale.na.fill(value=0).show()   \n",
    "    \n",
    "    return saffir_simpson_scale\n",
    "        \n",
    "    # write to s3 as csv\n",
    "    saffir_simpson_scale.write.mode(\"overwrite\") \\\n",
    "                     .csv('s3://'+SOURCE_BUCKET+'/OUTPUT/saffir_simpson_scale/saffir_simpson_scale.csv')     \n",
    "        \n",
    "            \n",
    "def process_state_ref(spark,SOURCE_BUCKET=''):\n",
    "    \"\"\"\n",
    "    Leverages readStateData as a helper function to read state \\\n",
    "    data from an existing s3 bucket and perform a number of data\\\n",
    "    transformations then output the data as a Pyspark dataframe.\n",
    "\n",
    "    A data is further processed to create a reference table \n",
    "    of the snowflake schema.\n",
    "\n",
    "    args:\n",
    "    spark = spark session parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # call helper\n",
    "    state_ref = readStateData(SOURCE_BUCKET)\n",
    "\n",
    "    # write to s3 as csv\n",
    "    state_ref.write.mode(\"overwrite\") \\\n",
    "                     .csv('s3a://'+SOURCE_BUCKET+'/OUTPUT/state_ref/state_ref.csv')\n",
    "\n",
    "def process_babyName_fact(spark,SOURCE_BUCKET=''):\n",
    "    \"\"\"\n",
    "    Leverages readBabyNameData as a helper function to read \\\n",
    "    data from an existing s3 bucket and perform a number of data\\\n",
    "    transformations then output the data as a Pyspark dataframe.\n",
    "\n",
    "    A data is further processed to create a fact table \n",
    "    of the snowflake schema.\n",
    "\n",
    "    args:\n",
    "    spark = spark session parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # call helper\n",
    "    babyNames_byState_fact = readBabyNameData(SOURCE_BUCKET)\n",
    "\n",
    "    # write to s3 as csv\n",
    "    babyNames_byState_fact.write.mode(\"overwrite\") \\\n",
    "                     .csv('s3a://'+SOURCE_BUCKET+'/OUTPUT/babyNames_byState_fact/babyNames_byState_fact.csv')\n",
    "    return babyNames_byState_fact\n",
    "\n",
    "def process_stormsByName_dim(spark,SOURCE_BUCKET=''):\n",
    "    \"\"\"\n",
    "    Leverages readStormData as a helper function to read \\\n",
    "    data from an existing s3 bucket and perform a number of data\\\n",
    "    transformations then output the data as a Pyspark dataframe.\n",
    "\n",
    "    A data is further processed to create a dim table \n",
    "    of the snowflake schema.\n",
    "\n",
    "    args:\n",
    "    spark = spark session parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # call helper\n",
    "    storms_byName_dim = readStateData(SOURCE_BUCKET) \n",
    "    \n",
    "    # write to s3 as csv\n",
    "    storms_byName_dim.write.mode(\"overwrite\") \\\n",
    "                     .csv('s3a://'+SOURCE_BUCKET+'/OUTPUT/storms_byName_dim/storms_byName_dim.csv')\n",
    "\n",
    "    \n",
    "def process_stormsLocation_dim(spark,SOURCE_BUCKET=''):\n",
    "    \"\"\"\n",
    "    Leverages readStormData as a helper function to read \\\n",
    "    data from an existing s3 bucket and perform a number of data\\\n",
    "    transformations then output the data as a Pyspark dataframe.\n",
    "\n",
    "    A data is further processed to create a dim table \n",
    "    of the snowflake schema.\n",
    "\n",
    "    args:\n",
    "    spark = spark session parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # call helper\n",
    "    storms_location_dim = readStormData(SOURCE_BUCKET)  \n",
    "    # select named storm that have storm_state_codes\n",
    "    storms_location_dim = storms_location_dim \\\n",
    "    .select(['storm_id', \n",
    "            'storm_name',\n",
    "            'storm_year',\n",
    "            'storm_type',\n",
    "            'storm_state_code' \n",
    "            ]) \\\n",
    "    .where(storms_location_dim['storm_state_code'].isNotNull())\n",
    "    \n",
    "    # groupby the max storm_year and drop dups\n",
    "    storms_location_dim = storms_location_dim \\\n",
    "        .groupBy(['storm_id',\n",
    "                 'storm_name',\n",
    "                 'storm_type',\n",
    "                 'storm_state_code'\n",
    "                 ]).agg(F.max(\"storm_year\"))\\\n",
    "        .dropDuplicates() \n",
    "    \n",
    "    # create an 'index'\n",
    "    storms_location_dim = storms_location_dim \\\n",
    "        .withColumn(\"location_id\", \\\n",
    "                    F.monotonically_increasing_id())\n",
    "\n",
    "    win = W.orderBy(\"location_id\")\n",
    "\n",
    "    storms_location_dim = storms_location_dim \\\n",
    "        .withColumn(\"location_id\", \\\n",
    "                    F.row_number().over(win)-1)\n",
    "    \n",
    "    # reorder\n",
    "    storms_location_dim = storms_location_dim \\\n",
    "        .select([\n",
    "            'location_id', \n",
    "            'storm_id',\n",
    "            'storm_name',\n",
    "            'max(storm_year)',\n",
    "            'storm_type',\n",
    "            'storm_state_code' \\\n",
    "        ])\n",
    "\n",
    "    # write to s3 as parquet\n",
    "    storms_location_dim.write.mode(\"overwrite\") \\\n",
    "                     .csv('s3a://'+SOURCE_BUCKET+'/OUTPUT/storms_location_dim/storms_location_dim.csv')    \n",
    "    \n",
    "\n",
    "def process_stormsSeverity_dim(spark,SOURCE_BUCKET=''):\n",
    "    \"\"\"\n",
    "    Leverages readStormData as a helper function to read \\\n",
    "    data from an existing s3 bucket and perform a number of data\\\n",
    "    transformations then output the data as a Pyspark dataframe.\n",
    "\n",
    "    A data is further processed to create a dim table \n",
    "    of the snowflake schema.\n",
    "\n",
    "    args:\n",
    "    spark = spark session parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # call helper\n",
    "    storms_severity_dim = readStormData(SOURCE_BUCKET)\n",
    "    \n",
    "    # select named storms with catergories\n",
    "    storms_severity_dim = storms_severity_dim \\\n",
    "        .select(['storm_id', \n",
    "                 'storm_name',\n",
    "                 'storm_year',\n",
    "                 'category',\n",
    "                'max_sustained_wind(kt)'         \n",
    "                ]) \\\n",
    "        .dropDuplicates()\n",
    "    \n",
    "    # groupby the max max_winds and drop dups\n",
    "    storms_severity_dim = storms_severity_dim \\\n",
    "        .groupBy(['storm_id',\n",
    "                 'storm_name',\n",
    "                 'storm_year',\n",
    "                 'category' \n",
    "                 ]).agg(F.max(\"max_sustained_wind(kt)\")) \\\n",
    "        .dropDuplicates()\n",
    "    \n",
    "    # create an 'index'\n",
    "    storms_severity_dim = storms_severity_dim \\\n",
    "        .withColumn(\"severity_id\", \\\n",
    "                    F.monotonically_increasing_id())\n",
    "\n",
    "    win = W.orderBy(\"severity_id\")\n",
    "\n",
    "    storms_severity_dim = storms_severity_dim \\\n",
    "        .withColumn(\"severity_id\", \\\n",
    "                    F.row_number().over(win)-1)\n",
    "    \n",
    "    # reorder\n",
    "    storms_severity_dim = storms_severity_dim \\\n",
    "        .select(['severity_id', \n",
    "                 'storm_id',\n",
    "                 'storm_name',\n",
    "                 'storm_year',\n",
    "                 'category',\n",
    "                 col('max(max_sustained_wind(kt))').alias('max_sustained_wind(kt)') \\\n",
    "                ])\n",
    "\n",
    "     # write to s3 as csv\n",
    "    storms_severity_dim.write.mode(\"overwrite\") \\\n",
    "                     .csv('s3a://'+SOURCE_BUCKET+'/OUTPUT/storms_severity_dim/storms_severity_dim.csv')    \n",
    "    \n",
    "\n",
    "def process_stormsMetadata_fact(spark,SOURCE_BUCKET=''):\n",
    "    \"\"\"\n",
    "    Leverages readStormData as a helper function to read \\\n",
    "    data from an existing s3 bucket and perform a number of data\\\n",
    "    transformations then output the data as a Pyspark dataframe.\n",
    "\n",
    "    A data is further processed to create a dim table \n",
    "    of the snowflake schema.\n",
    "\n",
    "    args:\n",
    "    spark = spark session parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # call helper\n",
    "    storms_metadata_fact = readStormData(SOURCE_BUCKET)\n",
    "    # groupby category\n",
    "    storms_metadata_fact = storms_metadata_fact \\\n",
    "        .groupBy(['storm_id',\n",
    "                 'storm_name',\n",
    "                 'storm_year',\n",
    "                 #'severity_id',there is no severity_id in storms_data.csv\n",
    "                 #'location_id',there is no location_id in storms_data.csv\n",
    "                 'storm_state_code' \n",
    "                 ]).agg(F.max(\"category\"))\\\n",
    "        .dropDuplicates()\n",
    "    \n",
    "    # create an 'index'\n",
    "    storms_metadata_fact = storms_metadata_fact \\\n",
    "        .withColumn(\"storm_meta_id\", \\\n",
    "                    F.monotonically_increasing_id())\n",
    "\n",
    "    win = W.orderBy(\"storm_meta_id\")\n",
    "\n",
    "    storms_metadata_fact = storms_metadata_fact \\\n",
    "        .withColumn(\"storm_meta_id\", \\\n",
    "                    F.row_number().over(win)-1)\n",
    "    \n",
    "    # reorder and alias\n",
    "    storms_metadata_fact = storms_metadata_fact \\\n",
    "    .select(['storm_meta_id', \n",
    "             'storm_id',\n",
    "             'storm_name',\n",
    "             'storm_year',\n",
    "             #'severity_id',there is no severity_id in storms_data.csv\n",
    "             col('max(category)').alias('category'),\n",
    "             #'location_id',there is no location_id in storms_data.csv\n",
    "             'storm_state_code'\n",
    "            ])\n",
    "    \n",
    "    # write to s3 as csv\n",
    "    storms_metadata_fact.write.mode(\"overwrite\") \\\n",
    "                     .csv('s3a://'+SOURCE_BUCKET+'/OUTPUT/storms_metadata_fact/storms_metadata_fact.csv')    \n",
    "    return  storms_metadata_fact\n",
    "\n",
    "def process_stormsBabyNames_fact(spark,SOURCE_BUCKET='',storms_metadata_fact='',babyNames_byState_fact=''):\n",
    "    \"\"\"\n",
    "    Leverages readStormData as a helper function to read \\\n",
    "    data from an existing s3 bucket and perform a number of data\\\n",
    "    transformations then output the data as a Pyspark dataframe.\n",
    "\n",
    "    A data is further processed to create a dim table \n",
    "    of the snowflake schema.\n",
    "\n",
    "    args:\n",
    "    spark = spark session parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # call helper\n",
    "    storms_babyNames_fact = readStormData(SOURCE_BUCKET)\n",
    "    \n",
    "    # join storms metadata fact table to baby names fact\n",
    "    meta = storms_metadata_fact \\\n",
    "        .select(['storm_meta_id',\n",
    "                'storm_id',\n",
    "                'storm_name',\n",
    "                'storm_year',\n",
    "                'storm_state_code',\n",
    "                'category',\n",
    "                'storm_meta_id',\n",
    "                'storm_id',\n",
    "                #'location_id',\n",
    "                #'severity_id'\n",
    "                ])\n",
    "    print(babyNames_byState_fact.columns)\n",
    "    bby = babyNames_byState_fact \\\n",
    "          .select(['babyName_id', \n",
    "                  'birth_name',\n",
    "                  'birth_year',\n",
    "                  'birth_state_code',\n",
    "                  'popularity',\n",
    "                  'gender'\n",
    "                ])\n",
    "\n",
    "\n",
    "    on = [(meta.storm_name == bby.birth_name)]\n",
    "    storms_babyNames_fact = bby \\\n",
    "                .join(broadcast(meta), on, 'inner') \\\n",
    "                .select(meta['storm_meta_id']\n",
    "                        ,bby['babyName_id']\\\n",
    "                        ,meta['storm_id'] \\\n",
    "                        ,meta['storm_name'] \\\n",
    "                        ,bby['birth_name'] \\\n",
    "                        ,bby['gender'] \\\n",
    "                        ,bby['birth_year'] \\\n",
    "                        ,bby['popularity'] \\\n",
    "                        ,meta['storm_year'] \\\n",
    "                        ,meta['category'] \\\n",
    "                        ,meta['storm_state_code'] \\\n",
    "                        ,bby['birth_state_code'] \\\n",
    "                       ) \\\n",
    "                .dropDuplicates()\n",
    "    \n",
    "    # create an 'index'\n",
    "    storms_babyNames_fact = storms_babyNames_fact \\\n",
    "        .withColumn(\"storm_babyName_id\", \\\n",
    "                    F.monotonically_increasing_id())\n",
    "\n",
    "    win = W.orderBy(\"storm_babyName_id\")\n",
    "\n",
    "    storms_babyNames_fact = storms_babyNames_fact \\\n",
    "        .withColumn(\"storm_babyName_id\", \\\n",
    "                    F.row_number().over(win)-1)\n",
    "    \n",
    "    # reorder \n",
    "    print(storms_babyNames_fact.columns)\n",
    "    storms_babyNames_fact = storms_babyNames_fact \\\n",
    "    .select(['storm_babyName_id',\n",
    "            'babyName_id',\n",
    "            'storm_meta_id',\n",
    "            'storm_id',\n",
    "            'storm_name',\n",
    "            'storm_year',\n",
    "            'category',\n",
    "            'storm_state_code',\n",
    "            'birth_year',\n",
    "            'birth_name',\n",
    "            'popularity',\n",
    "            'birth_state_code',\n",
    "            'gender'\n",
    "            ]).dropDuplicates()\n",
    "    \n",
    "    # write to s3 as csv\n",
    "    storms_babyNames_fact.write.mode(\"overwrite\") \\\n",
    "                     .csv('s3a://'+SOURCE_BUCKET+'/OUTPUT/storms_babyNames_fact/storms_babyNames_fact.csv')    \n",
    "    \n",
    "    \n",
    "def delete_S3(s3_resource, BUCKET):\n",
    "    \"\"\"\n",
    "    using boto3 s3 resource, empties the contents of an S3 bucket\n",
    "    then deletes the bucket\n",
    "    \n",
    "    args:\n",
    "    s3_resource = boto3 resource for s3\n",
    "    bucket = s3 bucket to empty and delete\n",
    "    \"\"\"\n",
    "    try:\n",
    "        deleteS3 = input('Ready to delete your S3 Bucket? Please answer: Yes or No...').lower()\n",
    "        if deleteS3.startswith('y'):\n",
    "            # to use .Bucket, the boto3 resource must be used\n",
    "            s3_bucket = s3_resource.Bucket(BUCKET)\n",
    "            \n",
    "            s3_bucket.objects.all().delete()\n",
    "            print('Bucket:', BUCKET, 'has been emptied')\n",
    "            \n",
    "            s3_bucket.delete()\n",
    "            print('Bucket:', BUCKET, 'has been deleted')\n",
    "        else: \n",
    "            print(\"Okay. Maybe later then.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "session = createBoto3Session()\n",
    "s3_client = createSessionClients(session,'s3', REGION)\n",
    "s3_resource = createSessionResources(session, 's3', REGION)\n",
    "create_new_S3(s3_client, SOURCE_BUCKET, REGION)\n",
    "upload_multiple_files_toS3Folder(s3_client, 'source_namesbystate/', '*.txt', SOURCE_BUCKET, 'SOURCE/')\n",
    "upload_multiple_files_toS3Folder(s3_client, 'source_weather/', '*.csv', SOURCE_BUCKET, 'SOURCE/')\n",
    "upload_multiple_files_toS3Folder(s3_client, 'source_states/', '*.json', SOURCE_BUCKET, 'SOURCE/')\n",
    "spark = create_spark_session()\n",
    "region = config.get(\"S3\", 'region')\n",
    "starttime = time.time()\n",
    "print(\"Start process_state_ref\")\n",
    "process_state_ref(spark,SOURCE_BUCKET)\n",
    "print(\"Start process_babyName_fact\")\n",
    "babyNames_byState_fact = process_babyName_fact(spark,SOURCE_BUCKET)\n",
    "print(\"Start process_stormsByName_dim\")\n",
    "process_stormsByName_dim(spark,SOURCE_BUCKET)\n",
    "print(\"Start process_stormsLocation_dim\")\n",
    "process_stormsLocation_dim(spark,SOURCE_BUCKET)\n",
    "print(\"Start process_stormsSeverity_dim\")\n",
    "process_stormsSeverity_dim(spark,SOURCE_BUCKET)\n",
    "print(\"Start process_stormsMetadata_fact\")\n",
    "storms_metadata_fact = process_stormsMetadata_fact(spark,SOURCE_BUCKET)\n",
    "print(\"Start process_stormsBabyNames_fact\")\n",
    "process_stormsBabyNames_fact(spark,SOURCE_BUCKET,storms_metadata_fact,babyNames_byState_fact)\n",
    "print(\"Complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
